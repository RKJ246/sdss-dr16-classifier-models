#################### sdss dr16 pca analysis v7


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
)
import xgboost as xgb
import csv
import itertools
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import re



############ TH optimize function

def find_best_threshold(y_true_val, y_proba_val):
    
    thresholds = np.linspace(0.01, 0.99, 99)
    best_f1, best_th = -1, 0.5
    for th in thresholds:

        f1 = f1_score(y_true_val, (y_proba_val >= th).astype(int), zero_division=0)
        if f1 > best_f1:
            best_f1, best_th = f1, th
    return best_th


file_path = r"C:\Users\Joshi Sir\Downloads\quasar_SDSS_DR16_500k_entries.csv"
n_rows_to_read = 500000


with open(file_path, 'r', encoding='utf-8') as f:
    sample = f.read(2048)
    sniffer = csv.Sniffer()
    try:
        delimiter = sniffer.sniff(sample).delimiter
    except csv.Error:
        delimiter = ','

data = pd.read_csv(file_path, sep=delimiter, low_memory=False, nrows=n_rows_to_read)



############### kellerman lable 

eps = 1e-10

data = data[data['first_flux'].notnull() & data['psfmag_i'].notnull()].copy()
data['first_flux_jy'] = data['first_flux'] * 1e-3
data['f_optical'] = 10 ** (-0.4 * (data['psfmag_i'] - 8.9))
data['R_i'] = data['first_flux_jy'] / (data['f_optical'] + eps)
data['Radio_Loud'] = (data['R_i'] > 10).astype(int)

num_loud = data['Radio_Loud'].sum()
num_quiet = (data['Radio_Loud'] == 0).sum()
total = len(data)

print("\n===== DATA OVERVIEW =====")
print(f"Total quasars read: {total:,}")
print(f"Radio-Loud quasars: {num_loud:,}")
print(f"Radio-Quiet quasars: {num_quiet:,}")
if total > 0:
    print(f"Radio-Loud fraction: {num_loud / total:.4f}")

    
    
################ features

eps = 1e-9

for col in ['rosat_src_flux', 'xmm_total_flux']:
    if col in data.columns:
        nonpos_mask = (data[col] <= 0) & (~data[col].isnull())
        if nonpos_mask.any():
            data.loc[nonpos_mask, col] = eps
        data[col] = np.log10(data[col])

for a, b, name in [
    ('psfmag_u', 'psfmag_g', 'ug'),
    ('psfmag_g', 'psfmag_r', 'gr'),
    ('psfmag_r', 'psfmag_i', 'ri'),
    ('psfmag_i', 'psfmag_z', 'iz'),
    ('w1_mag', 'w2_mag', 'w1w2')
]:
    if (a in data.columns) and (b in data.columns):
        data[name] = data[a] - data[b]

features = [
    'psfmag_u', 'psfmag_g', 'psfmag_r', 'psfmag_i', 'psfmag_z',
    'w1_mag', 'w2_mag',
    'w1_flux_snr', 'w2_flux_snr',
    'rosat_src_flux',
    'mi', 'z_pca', 'sn_median_all',
    'ug', 'gr', 'ri', 'iz', 'w1w2'
]
features = [f for f in features if f in data.columns]
data = data[features + ['Radio_Loud']].replace([np.inf, -np.inf], np.nan).dropna()


############### train test val split

X = data[features]
y = data['Radio_Loud']

# Split data into Training_Val (70%) and Test (30%)
train_val_idx, test_idx = train_test_split(
    np.arange(len(X)), test_size=0.3, random_state=42, stratify=y
)
X_train_val = X.iloc[train_val_idx].reset_index(drop=True)
y_train_val = y.iloc[train_val_idx].reset_index(drop=True)
X_test = X.iloc[test_idx].reset_index(drop=True)
y_test = y.iloc[test_idx].reset_index(drop=True)

# Split Training_Val (70%) into Training (70% of 70%) and Validation (30% of 70%)
train_idx_split, val_idx_split = train_test_split(
    np.arange(len(X_train_val)), test_size=0.3, random_state=42, stratify=y_train_val
)
X_train = X_train_val.iloc[train_idx_split].reset_index(drop=True)
X_val = X_train_val.iloc[val_idx_split].reset_index(drop=True)
y_train = y_train_val.iloc[train_idx_split].reset_index(drop=True)
y_val = y_train_val.iloc[val_idx_split].reset_index(drop=True)

print(f"\n--- Data Split ---")
print(f"Training set size: {len(X_train):,}")
print(f"Validation set size: {len(X_val):,}")
print(f"Test set size: {len(X_test):,}")

################## pca +scaling routine

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

pca = PCA(n_components=7, random_state=42)
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
X_test_pca = pca.transform(X_test_scaled)
print("\nPCA variance ratios (first 7):", np.round(pca.explained_variance_ratio_[:7], 4))



############# rf and xgb models with optimal TH

###### rf

rf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')
rf.fit(X_train_scaled, y_train)

y_proba_rf_val = rf.predict_proba(X_val_scaled)[:, 1]
y_proba_rf = rf.predict_proba(X_test_scaled)[:, 1]


best_th_rf = find_best_threshold(y_val, y_proba_rf_val)
y_pred_rf = (y_proba_rf >= best_th_rf).astype(int)
print(f"\nRF Base Best Threshold (from Val Set): {best_th_rf:.4f}")

################# xgb

scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]
xgb_clf = xgb.XGBClassifier(
    n_estimators=300, learning_rate=0.05, max_depth=6,
    subsample=0.8, colsample_bytree=0.8, random_state=42,
    use_label_encoder=False, eval_metric='auc',
    scale_pos_weight=scale_pos_weight
)
xgb_clf.fit(X_train_scaled, y_train)


y_proba_xgb_val = xgb_clf.predict_proba(X_val_scaled)[:, 1]
y_proba_xgb = xgb_clf.predict_proba(X_test_scaled)[:, 1]


best_th_xgb = find_best_threshold(y_val, y_proba_xgb_val)
y_pred_xgb = (y_proba_xgb >= best_th_xgb).astype(int)
print(f"XGB Base Best Threshold (from Val Set): {best_th_xgb:.4f}")



################# lrc routine on pca

results = []
pairs = list(itertools.combinations(range(X_train_pca.shape[1]), 2))
print(f"\nRunning PCA + LR analysis on {len(pairs)} PCA component pairs...\n")

for (i, j) in pairs:
    X_train_pair = X_train_pca[:, [i, j]]
    X_val_pair = X_val_pca[:, [i, j]]
    X_test_pair = X_test_pca[:, [i, j]]

    scaler_pca = StandardScaler()
    X_train_scaled_pair = scaler_pca.fit_transform(X_train_pair)
    X_val_scaled_pair = scaler_pca.transform(X_val_pair)
    X_test_scaled_pair = scaler_pca.transform(X_test_pair)

    poly = PolynomialFeatures(degree=2, include_bias=False)              ############## CHANGE DEGREE HERE
    X_train_poly = poly.fit_transform(X_train_scaled_pair)
    X_val_poly = poly.transform(X_val_scaled_pair)
    X_test_poly = poly.transform(X_test_scaled_pair)


    lr_pca = LogisticRegression(
        max_iter=1000, random_state=42, solver='liblinear',
        class_weight='balanced'
    )
    lr_pca.fit(X_train_poly, y_train)

    
    y_proba_lr_val = lr_pca.predict_proba(X_val_poly)[:, 1]
    y_proba_lr = lr_pca.predict_proba(X_test_poly)[:, 1]

    
    best_th_lr = find_best_threshold(y_val, y_proba_lr_val)
    y_pred_lr = (y_proba_lr >= best_th_lr).astype(int)


    recall_val = recall_score(y_test, y_pred_lr)
    precision_val = precision_score(y_test, y_pred_lr, zero_division=0)

    correct_loud = ((y_test == 1) & (y_pred_lr == 1)).sum() 
    predicted_loud = (y_pred_lr == 1).sum()
    total_loud = (y_test == 1).sum()

    fpr, tpr, _ = roc_curve(y_test, y_proba_lr)
    auc_val = auc(fpr, tpr)

    results.append({
        "PC Pair": f"PC{i+1}-PC{j+1}",
        "Accuracy": accuracy_score(y_test, y_pred_lr),
        "Precision": precision_val,
        "Recall": recall_val,
        "F1": f1_score(y_test, y_pred_lr),
        "AUC": auc_val,
        "Predicted Loud": predicted_loud,
        "Correct Loud": correct_loud,
        "Total Loud": total_loud,
        "Threshold": best_th_lr
    })


summary_df = pd.DataFrame(results).set_index("PC Pair")
summary_sorted = summary_df.sort_values(by="Recall", ascending=False)
best_pca = summary_sorted.iloc[0]

print("\n===== PCA + LRc summary =====")
print(summary_sorted.round(4))

num_true_rlqs = y_test.sum()
correct_rf = ((y_test == 1) & (y_pred_rf == 1)).sum()
correct_xgb = ((y_test == 1) & (y_pred_xgb == 1)).sum()

print("\n===== correct rlqs on Test Set =====")
print(f"Total True RLQS in Test Set: {num_true_rlqs:,}")
print(f"RF Base (Class Weighted, Th={best_th_rf:.4f}): {correct_rf:,}")
print(f"XGB Base (Scale Pos Weighted, Th={best_th_xgb:.4f}): {correct_xgb:,}")
print(f"Best PCA+LR ({best_pca.name}, Th={best_pca['Threshold']:.4f}): {int(best_pca['Correct Loud']):,}")


##################### roc and confusion matrices for best pca pair
best_pair = best_pca.name
nums = re.findall(r'\d+', best_pair)
i_best, j_best = map(int, nums)


X_test_pair_best = X_test_pca[:, [i_best-1, j_best-1]]
X_train_pair_best = X_train_pca[:, [i_best-1, j_best-1]]

scaler_best = StandardScaler().fit(X_train_pair_best)
poly_best = PolynomialFeatures(degree=2, include_bias=False)               ################ CHANGE DEGREE HERE


X_train_poly_best = poly_best.fit_transform(scaler_best.transform(X_train_pair_best))
X_test_poly_best = poly_best.transform(scaler_best.transform(X_test_pair_best))

lr_best_final = LogisticRegression(
    max_iter=1000, random_state=42, solver='liblinear',
    class_weight='balanced'
)
lr_best_final.fit(X_train_poly_best, y_train) 
y_proba_lr_best = lr_best_final.predict_proba(X_test_poly_best)[:, 1]
y_pred_lr_best = (y_proba_lr_best >= best_pca['Threshold']).astype(int) 

fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr_best)
auc_lr = auc(fpr_lr, tpr_lr)

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_proba_xgb)
auc_rf = auc(fpr_rf, tpr_rf)
auc_xgb = auc(fpr_xgb, tpr_xgb)

plt.figure(figsize=(8,6))
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC={auc_rf:.3f})', lw=2)
plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC={auc_xgb:.3f})', lw=2)
plt.plot(fpr_lr, tpr_lr, label=f'Best PCA ({best_pair}) (AUC={auc_lr:.3f})', lw=2)
plt.plot([0,1],[0,1],'k--', lw=1)
plt.xlabel("False Positive Rate",fontsize=16)
plt.ylabel("True Positive Rate",fontsize=16)
plt.legend(fontsize=10,loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show() # 

models = {
    f"RF (Th={best_th_rf:.2f})": (y_test, y_pred_rf),
    f"XGB (Th={best_th_xgb:.2f})": (y_test, y_pred_xgb),
    f"Best PCA ({best_pair}, Th={best_pca['Threshold']:.2f})": (y_test, y_pred_lr_best)
}

for name, (y_true, y_pred) in models.items():
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(cm, display_labels=["Quiet", "Loud"])
    disp.plot(cmap="Blues", values_format="d")
    plt.title(f"Confusion Matrix: {name}", fontsize=16, fontweight='bold')
    plt.xlabel("Predicted Label", fontsize=16)
    plt.ylabel("True Label", fontsize=16)
    plt.xticks(fontsize=14, fontweight='bold')
    plt.yticks(fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show() # 


plt.figure(figsize=(10,8))
for row in results:
    pair_name = row["PC Pair"]
    nums = re.findall(r'\d+', pair_name)
    i, j = map(int, nums)

    X_test_pair_tmp = X_test_pca[:, [i-1, j-1]]
    X_train_pair_tmp = X_train_pca[:, [i-1, j-1]]

    scaler_tmp = StandardScaler().fit(X_train_pair_tmp)
    poly_tmp = PolynomialFeatures(degree=2, include_bias=False)            ############ CHANGE DEGREE HERE
     

    X_poly_train_tmp = poly_tmp.fit_transform(scaler_tmp.transform(X_train_pair_tmp))
    X_poly_test_tmp = poly_tmp.transform(scaler_tmp.transform(X_test_pair_tmp))

    lr_tmp = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced')
    lr_tmp.fit(X_poly_train_tmp, y_train)

    y_proba_tmp = lr_tmp.predict_proba(X_poly_test_tmp)[:, 1]
    fpr_tmp, tpr_tmp, _ = roc_curve(y_test, y_proba_tmp)
    auc_tmp = auc(fpr_tmp, tpr_tmp)
    plt.plot(fpr_tmp, tpr_tmp, lw=1, alpha=0.6, label=f"{pair_name} (AUC={auc_tmp:.2f})")

plt.plot([0,1],[0,1],'k--', lw=1)
plt.xlabel("False Positive Rate",fontsize=16)
plt.ylabel("True Positive Rate",fontsize=16)
plt.legend(fontsize=10, loc="lower right", ncol=2)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show() # 

################# top pc pair visualization

top3 = summary_df.sort_values(by="Correct Loud", ascending=False).head(3)
fig, axes = plt.subplots(3, 4, figsize=(22, 16))
axes = axes.reshape(3, 4)

# axis_limits = {
#      ('PC1', 'PC3'): {'xlim': (-10, 10), 'ylim': (-20, 20)},          ############ use manual limits for better visualisation
#      ('PC1', 'PC6'): {'xlim': (-10, 10), 'ylim': (-25, 25)},
#      ('PC1', 'PC5'): {'xlim': (-15, 15), 'ylim': (-25, 25)},
# }

for row_idx, (pair_name, row) in enumerate(top3.iterrows()):
    nums = re.findall(r'\d+', pair_name)
    i, j = map(int, nums)
    i -= 1; j -= 1

    pcx, pcy = f"PC{i+1}", f"PC{j+1}"
    X_train_pair = X_train_pca[:, [i, j]]
    mask_loud = (y_train == 1)
    mask_quiet = (y_train == 0)


    if (pcx, pcy) in axis_limits:
        x_low, x_high = axis_limits[(pcx, pcy)]['xlim']
        y_low, y_high = axis_limits[(pcx, pcy)]['ylim']
    else:
        margin = 0.1
        x_min, x_max = X_train_pair[:, 0].min(), X_train_pair[:, 0].max()
        y_min, y_max = X_train_pair[:, 1].min(), X_train_pair[:, 1].max()
        x_low, x_high = x_min - margin*(x_max - x_min), x_max + margin*(x_max - x_min)
        y_low, y_high = y_min - margin*(y_max - y_min), y_max + margin*(y_max - y_min)

    inlier_mask = (
        (X_train_pair[:, 0] >= x_low) & (X_train_pair[:, 0] <= x_high) &
        (X_train_pair[:, 1] >= y_low) & (X_train_pair[:, 1] <= y_high)
    )

    X_inlier = X_train_pair[inlier_mask]
   
    y_inlier = y_train.loc[y_train.index[inlier_mask]] 
    mask_loud_inlier = (y_inlier == 1)
    mask_quiet_inlier = (y_inlier == 0)

    ############## lrc fitting - decision boundary
    
    scaler_pca = StandardScaler().fit(X_inlier)
    poly = PolynomialFeatures(degree=2, include_bias=False)      ############## CHANGE DEGREE HERE
    X_poly = poly.fit_transform(scaler_pca.transform(X_inlier))
    lr = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced')
    lr.fit(X_poly, y_inlier)

    xx, yy = np.meshgrid(
        np.linspace(x_low, x_high, 300),
        np.linspace(y_low, y_high, 300)
    )
    grid = np.c_[xx.ravel(), yy.ravel()]
    probs = lr.predict_proba(poly.transform(scaler_pca.transform(grid)))[:, 1].reshape(xx.shape)

    # rl+rq
    ax = axes[row_idx, 0]
    sns.scatterplot(x=X_inlier[:, 0], y=X_inlier[:, 1],
                    hue=y_inlier.values, palette=['blue', 'red'], alpha=0.5, s=15, ax=ax, legend=False)
    ax.set_title(f"{pair_name} | RL+RQ", fontsize=19)
    ax.set_xlabel(f"PC{i+1}", fontsize=24)
    ax.set_ylabel(f"PC{j+1}", fontsize=24)
    ax.set_xlim(x_low, x_high); ax.set_ylim(y_low, y_high)

    # rl only
    ax = axes[row_idx, 1]
    sns.scatterplot(x=X_inlier[mask_loud_inlier.values, 0], y=X_inlier[mask_loud_inlier.values, 1],
                    color='red', alpha=0.6, s=15, ax=ax)
    ax.set_title("Radio-Loud Only", fontsize=19)
    ax.set_xlabel(f"PC{i+1}", fontsize=24)
    ax.set_ylabel(f"PC{j+1}", fontsize=24)
    ax.set_xlim(x_low, x_high); ax.set_ylim(y_low, y_high)

    # rq only
    ax = axes[row_idx, 2]
    sns.scatterplot(x=X_inlier[mask_quiet_inlier.values, 0], y=X_inlier[mask_quiet_inlier.values, 1],
                    color='blue', alpha=0.6, s=15, ax=ax)
    ax.set_title("Radio-Quiet Only", fontsize=19)
    ax.set_xlabel(f"PC{i+1}", fontsize=24)
    ax.set_ylabel(f"PC{j+1}", fontsize=24)
    ax.set_xlim(x_low, x_high); ax.set_ylim(y_low, y_high)

    # decision boundary
    ax = axes[row_idx, 3]
    sns.scatterplot(x=X_inlier[:, 0], y=X_inlier[:, 1],
                    hue=y_inlier.values, palette=['blue', 'red'], alpha=0.4, s=15, ax=ax, legend=False)

    ax.contour(xx, yy, probs, levels=[row['Threshold']], colors='black', linestyles='--')
    ax.set_title(f"Decision Boundary | Correct RL={int(row['Correct Loud'])}", fontsize=19)
    ax.set_xlabel(f"PC{i+1}", fontsize=24)
    ax.set_ylabel(f"PC{j+1}", fontsize=24)
    ax.set_xlim(x_low, x_high); ax.set_ylim(y_low, y_high)

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show() # 

#for ax_row in axes:
#    for ax in ax_row:
#        ax.grid(False)
        
        
        
        
        
################### linear emcee corner plot v2
        
        
import numpy as np
import matplotlib.pyplot as plt
import corner
import emcee
from scipy.special import expit  


pc_i, pc_j = i_best-1, j_best-1   ####### by default this uses the best pc pair computed from the above routine(degree based)

X_pair = X_train_pca[:, [pc_i, pc_j]]
y_pair = y_train.values if hasattr(y_train, "values") else np.asarray(y_train)


X_mean = X_pair.mean(axis=0)
X_std  = X_pair.std(axis=0) + 1e-9
X_std_pair = (X_pair - X_mean) / X_std


def log_prior(theta):
    w0, w1, b = theta

    sigma_w = 2.0
    sigma_b = 5.0
    lp = -0.5*(w0**2 + w1**2) / sigma_w**2 - 0.5*(b**2) / sigma_b**2
    return lp

def log_likelihood(theta, X, y):
    w0, w1, b = theta
    lin = X[:,0]*w0 + X[:,1]*w1 + b
    p = expit(lin)

    eps = 1e-12
    ll = np.sum(y * np.log(p + eps) + (1-y) * np.log(1 - p + eps))
    return ll

def log_posterior(theta, X, y):
    lp = log_prior(theta)
    if not np.isfinite(lp):
        return -np.inf
    return lp + log_likelihood(theta, X, y)

########## setup
ndim = 3
nwalkers = 64

try:
    from sklearn.linear_model import LogisticRegression
    lr_tmp = LogisticRegression(solver='liblinear', class_weight='balanced', max_iter=2000)
    lr_tmp.fit(X_std_pair, y_pair)
    coefs = lr_tmp.coef_.ravel()
    intercept = lr_tmp.intercept_[0]
    p0_center = np.array([coefs[0], coefs[1], intercept])
except Exception:
    p0_center = np.zeros(ndim)

p0 = p0_center + 1e-3 * np.random.randn(nwalkers, ndim)

sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(X_std_pair, y_pair))
nsteps = 3000
print("Running emcee (gonna take long time my friend, hold tight)...")
sampler.run_mcmc(p0, nsteps, progress=True)


burn = int(nsteps * 0.4)
samples = sampler.get_chain(discard=burn, flat=True)
print("Posterior samples shape:", samples.shape)


labels = [f"w0 (PC{pc_i+1})", f"w1 (PC{pc_j+1})", "b (intercept)"]
fig = corner.corner(samples, labels=labels, quantiles=[0.16,0.5,0.84], show_titles=True,
                    title_kwargs={"fontsize":12}, label_kwargs={"fontsize":12})
#plt.suptitle(f"MCMC Posterior: Logistic regression on PC{pc_i+1}-PC{pc_j+1}", 
plt.show()


import numpy as np
for k, lab in enumerate(labels):
    m = np.percentile(samples[:,k], [16,50,84])
    median = m[1]
    minus = m[1]-m[0]
    plus = m[2]-m[1]
    print(f"{lab}: median={median:.3f} (+{plus:.3f}/-{minus:.3f})")

    
plt.figure(figsize=(8,6))

X_plot = X_pair.copy()
y_plot = y_pair

plt.scatter(X_plot[y_plot==0,0], X_plot[y_plot==0,1], s=10, alpha=0.3, color='blue', label='RQ')
plt.scatter(X_plot[y_plot==1,0], X_plot[y_plot==1,1], s=10, alpha=0.3, color='red', label='RL')


n_draws = 200
idxs = np.random.choice(samples.shape[0], size=n_draws, replace=False)
xvals = np.linspace(X_plot[:,0].min(), X_plot[:,0].max(), 200)

# for idx in idxs:
#     w0_s, w1_s, b_s = samples[idx]
# ########## convert back to pca space (potential visualization boundary shift)(not necessary for analysis)
#     slope = - (w0_s / X_std[0]) / (w1_s / X_std[1]) if abs(w1_s) > 1e-12 else np.nan
#     intercept = ( -b_s + (w0_s*X_mean[0]/X_std[0]) - (w1_s*X_mean[1]/X_std[1]) ) / (w1_s / X_std[1]) if abs(w1_s) > 1e-12 else np.nan
#     if np.isfinite(slope) and np.isfinite(intercept):
#         yline = slope * xvals + intercept
#         plt.plot(xvals, yline, color='k', alpha=0.02)

# plt.xlabel(f"PC{pc_i+1}", fontsize=14)
# plt.ylabel(f"PC{pc_j+1}", fontsize=14)
# #plt.title(f"Posterior decision-boundary samples (PC{pc_i+1}-PC{pc_j+1})", fontsize=16)
# plt.legend()
# plt.xlim(X_plot[:,0].min(), X_plot[:,0].max())
# plt.ylim(X_plot[:,1].min(), X_plot[:,1].max())
# plt.show()
        
        
        

        
        
        
        
        
        
        
###################### auadratic emcee v4

import numpy as np
import matplotlib.pyplot as plt
import corner
import emcee
from scipy.special import expit   # sigmoid

#################### manually choose pc indices

deg2_pc_i=0
deg2_pc_j=2


pc_i, pc_j = deg2_pc_i, deg2_pc_j

X_pair = X_train_pca[:, [pc_i, pc_j]]
y_pair = y_train.values if hasattr(y_train, "values") else np.asarray(y_train)


X_mean = X_pair.mean(axis=0)
X_std  = X_pair.std(axis=0) + 1e-9
X_std_pair = (X_pair - X_mean) / X_std



x = X_std_pair[:, 0]
y = X_std_pair[:, 1]
X_poly = np.column_stack([x, y, x*x, y*y, x*y])


ndim = 6

def log_prior(theta):
    sigma_w = 2.0
    sigma_b = 5.0
    w = theta[:-1]
    b = theta[-1]
    return -0.5 * np.sum(w**2) / sigma_w**2 - 0.5 * (b**2) / sigma_b**2


def log_likelihood(theta, X, y):
    w = theta[:-1]
    b = theta[-1]
    z = X.dot(w) + b
    p = expit(z)
    eps = 1e-12
    return np.sum(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))


def log_posterior(theta, X, y):
    lp = log_prior(theta)
    if not np.isfinite(lp):
        return -np.inf
    return lp + log_likelihood(theta, X, y)



############# setup

nwalkers = 64
p0 = 1e-3 * np.random.randn(nwalkers, ndim)

sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(X_poly, y_pair))

nsteps = 4000
print("Running EMCEE sampling for degree-2 model...")
sampler.run_mcmc(p0, nsteps, progress=True)

burn = int(nsteps * 0.4)
samples = sampler.get_chain(discard=burn, flat=True)
print("Posterior samples shape:", samples.shape)

labels = [
    f"w_x (PC{pc_i+1})",
    f"w_y (PC{pc_j+1})",
    "w_x2",
    "w_y2",
    "w_xy",
    "b (intercept)"
]

fig = corner.corner(samples, labels=labels, quantiles=[0.16, 0.5, 0.84],
                    show_titles=True, title_kwargs={"fontsize":12},
                    label_kwargs={"fontsize":12})
plt.show()


for k, lab in enumerate(labels):
    m = np.percentile(samples[:,k], [16, 50, 84])
    median = m[1]
    minus = m[1] - m[0]
    plus  = m[2] - m[1]
    print(f"{lab}: median={median:.3f} (+{plus:.3f}/-{minus:.3f})")

# ################ posterior boundary (optional)
# plt.figure(figsize=(8,6))
# plt.scatter(X_pair[y_pair==0,0], X_pair[y_pair==0,1], s=10, alpha=0.3, color='blue', label='RQ')
# plt.scatter(X_pair[y_pair==1,0], X_pair[y_pair==1,1], s=10, alpha=0.3, color='red', label='RL')

# # Grid (verify)
# x_vals = np.linspace(X_pair[:,0].min(), X_pair[:,0].max(), 200)
# y_vals = np.linspace(X_pair[:,1].min(), X_pair[:,1].max(), 200)
# xx, yy = np.meshgrid(x_vals, y_vals)


# xxs = (xx - X_mean[0]) / X_std[0]
# yys = (yy - X_mean[1]) / X_std[1]


# n_draws = 60
# idxs = np.random.choice(samples.shape[0], n_draws, replace=False)

# for idx in idxs:
#     w = samples[idx, :5]
#     b = samples[idx, 5]
#     z = (w[0]*xxs + w[1]*yys + w[2]*xxs**2 + w[3]*yys**2 + w[4]*xxs*yys + b)
#     plt.contour(xx, yy, z, levels=[0], colors='k', alpha=0.05)

# plt.xlabel(f"PC{pc_i+1}", fontsize=14)
# plt.ylabel(f"PC{pc_j+1}", fontsize=14)
# plt.legend()
# plt.show()

        
        
        
        
        
        
        
        
################################# KDE on top 3 pc pairs (degree dependent)

from scipy.stats import gaussian_kde
import numpy as np
import matplotlib.pyplot as plt

def run_kde_on_pca_pairs(X_pca, y, pc_pairs=None, bandwidth=None):



#     axis_limits = {
#         (5, 6): (-30, 55, -20, 30),   # PC6 vs PC7                        ############# manual limits if required
#         (0, 6): (-15, 25, -15, 30),   # PC1 vs PC7
#         (4, 6): (-25, 45, -15, 15),   # PC5 vs PC7
#     }
#     # =====================================================



    if pc_pairs is None:
        pc_pairs = summary_df.sort_values(by="Correct Loud", ascending=False).head(3).index
        parsed = []
        import re
        for pair_name in pc_pairs:
            nums = re.findall(r'\d+', pair_name)
            i, j = map(int, nums)
            parsed.append((i-1, j-1))
        pc_pairs = parsed
        

  ################## different train test ratios (kde * 3 runs per pc for each fraction)

    ratios = [0.3, 0.5, 0.8]
    linestyles = ['solid', 'dashed', 'dotted']

    for (i, j) in pc_pairs:

        pcx, pcy = f"PC{i+1}", f"PC{j+1}"
        X_pair = X_pca[:, [i, j]]
        rl = (y == 1)
        rq = (y == 0)

        fig, ax = plt.subplots(figsize=(9, 7))

       
        ax.scatter(X_pair[rq, 0], X_pair[rq, 1], s=6, alpha=0.35, color='blue', label='Radio-Quiet points')
        ax.scatter(X_pair[rl, 0], X_pair[rl, 1], s=6, alpha=0.35, color='red', label='Radio-Loud points')

       
        xx, yy = np.meshgrid(
            np.linspace(X_pair[:, 0].min(), X_pair[:, 0].max(), 300),
            np.linspace(X_pair[:, 1].min(), X_pair[:, 1].max(), 300)
        )

       
        for r, ls in zip(ratios, linestyles):

          
            n_rl = max(10, int(np.sum(rl) * r))
            n_rq = max(10, int(np.sum(rq) * r))

            X_rl_sub = X_pair[rl][:n_rl]
            X_rq_sub = X_pair[rq][:n_rq]

            kde_rl = gaussian_kde(X_rl_sub.T, bw_method=bandwidth)
            kde_rq = gaussian_kde(X_rq_sub.T, bw_method=bandwidth)

            zz_rl = kde_rl(np.vstack([xx.ravel(), yy.ravel()])).reshape(xx.shape)
            zz_rq = kde_rq(np.vstack([xx.ravel(), yy.ravel()])).reshape(xx.shape)

            
            cs_rl = ax.contour(xx, yy, zz_rl, levels=5, linewidths=1.6, colors='black')
            for c in cs_rl.collections:
                c.set_linestyle(ls)

            cs_rq = ax.contour(xx, yy, zz_rq, levels=5, linewidths=1.6, colors='green')
            for c in cs_rq.collections:
                c.set_linestyle(ls)

        if (i, j) in axis_limits:
            xmin, xmax, ymin, ymax = axis_limits[(i, j)]
            ax.set_xlim(xmin, xmax)
            ax.set_ylim(ymin, ymax)

   
        ax.set_xlabel(pcx, fontsize=18)
        ax.set_ylabel(pcy, fontsize=18)
        ax.set_title(f"KDE Density Hotspots: {pcx} vs {pcy}", fontsize=20)
        ax.grid(False)

     
        custom_lines = [
            plt.Line2D([0], [0], color='black', lw=2, linestyle='solid', label='RL KDE (30%)'),
            plt.Line2D([0], [0], color='black', lw=2, linestyle='dashed', label='RL KDE (50%)'),
            plt.Line2D([0], [0], color='black', lw=2, linestyle='dotted', label='RL KDE (80%)'),
            plt.Line2D([0], [0], color='green', lw=2, linestyle='solid', label='RQ KDE (30%)'),
            plt.Line2D([0], [0], color='green', lw=2, linestyle='dashed', label='RQ KDE (50%)'),
            plt.Line2D([0], [0], color='green', lw=2, linestyle='dotted', label='RQ KDE (80%)'),
            plt.Line2D([0], [0], marker='o', color='red', markersize=6, linestyle='None', label='Radio-Loud points'),
            plt.Line2D([0], [0], marker='o', color='blue', markersize=6, linestyle='None', label='Radio-Quiet points')
        ]

        ax.legend(handles=custom_lines, fontsize=13, loc='center left', bbox_to_anchor=(1.02, 0.5))
        # =================================================

        plt.tight_layout()
        plt.show()


run_kde_on_pca_pairs(X_train_pca, y_train)
